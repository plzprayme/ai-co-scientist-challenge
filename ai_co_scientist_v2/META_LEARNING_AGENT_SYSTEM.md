# 2026 AI Co-Scientist Challenge Korea - Track 1
# ë©”íƒ€ëŸ¬ë‹ ê¸°ë°˜ ìê¸°ê°œì„  ë¬´í•œë£¨í”„ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ (Meta-Learning Self-Improving Infinite Loop Agent System)

## 1. ê¸°ì¡´ ì‹œìŠ¤í…œ ë¹„íŒì  í‰ê°€ (Critical Analysis of Previous System)

### 1.1 ì£¼ìš” ë¬¸ì œì 

| ë¬¸ì œì  | ì‹¬ê°ë„ | ì„¤ëª… |
|--------|--------|------|
| **ì •ì  ì—ì´ì „íŠ¸ êµ¬ì¡°** | ğŸ”´ ë†’ìŒ | ì—ì´ì „íŠ¸ê°€ ê³ ì •ëœ ë¡œì§ìœ¼ë¡œ ë™ì‘í•˜ë©°, iterationì„ ê±°ì¹˜ë©° ìŠ¤ìŠ¤ë¡œ ê°œì„ ë˜ì§€ ì•ŠìŒ |
| **ë‹¨ì¼ í”¼ë“œë°± ë£¨í”„** | ğŸ”´ ë†’ìŒ | ì œì¶œë¬¼ë§Œ ê°œì„ ë˜ê³ , ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ìì²´ëŠ” ê°œì„ ë˜ì§€ ì•ŠìŒ |
| **No Version Control** | ğŸŸ¡ ì¤‘ê°„ | iterationë§ˆë‹¤ commitì´ ì—†ì–´ ì¶”ì  ë° ë¡¤ë°± ë¶ˆê°€ |
| **ì œí•œëœ AI í™œìš©** | ğŸŸ¡ ì¤‘ê°„ | Claudeë§Œ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°ë¡œ 3ê°œ ì´ìƒ ëª¨ë¸ í™œìš© ìš”êµ¬ì‚¬í•­ ë¯¸í¡ |
| **No Meta-Learning** | ğŸ”´ ë†’ìŒ | ì´ì „ iterationì˜ í•™ìŠµì´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ |
| **ë‹¨ìˆœ ì‹œë®¬ë ˆì´ì…˜** | ğŸ”´ ë†’ìŒ | ì‹¤ì œ ë°ì´í„° ë¶„ì„ì´ ì•„ë‹Œ mock ë°ì´í„° ì‚¬ìš© |

### 1.2 ëŒ€íšŒ ìš”êµ¬ì‚¬í•­ê³¼ì˜ ê´´ë¦¬

```
ëŒ€íšŒ ìš”êµ¬ì‚¬í•­                    ê¸°ì¡´ ì‹œìŠ¤í…œ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3ê°œ ì´ìƒ AI ëª¨ë¸ í™œìš©     â†’      ë‹¨ì¼ ëª¨ë¸ ì¤‘ì‹¬
ì‹¤ì œ ì—°êµ¬ ìˆ˜í–‰            â†’      Mock ë°ì´í„° ê¸°ë°˜
AI í™œìš©ë³´ê³ ì„œ (ìƒì„¸ ë¡œê·¸)  â†’      í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±
ì—°êµ¬ ì „ ê³¼ì • AI ê¸°ì—¬      â†’      ì¼ë¶€ ë‹¨ê³„ë§Œ AI í™œìš©
ì œì¶œë¬¼ í’ˆì§ˆ ê°œì„            â†’      ê³ ì •ëœ í‰ê°€ ê¸°ì¤€
```

---

## 2. ê°œì„ ëœ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜: MIRROR (Meta-Learning Iterative Research Optimization & Reflection System)

### 2.1 í•µì‹¬ ê°œë…: ì´ì¤‘ ë£¨í”„ í•™ìŠµ (Dual-Loop Learning)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MIRROR ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     ë©”íƒ€ëŸ¬ë‹ ë ˆì´ì–´ (Meta-Learning)   â”‚
                    â”‚  - Agent Architecture ê°œì„            â”‚
                    â”‚  - Prompt Strategy ìµœì í™”            â”‚
                    â”‚  - Workflow ì¬êµ¬ì„±                   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     ë¦¬í”Œë ‰ì…˜ ë ˆì´ì–´ (Reflection)      â”‚
                    â”‚  - iterationë³„ ì„±ëŠ¥ ë¶„ì„             â”‚
                    â”‚  - ì‹¤íŒ¨ ì›ì¸ ì§„ë‹¨                    â”‚
                    â”‚  - ê°œì„  ì „ëµ ìƒì„±                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                          â”‚                          â”‚
        â–¼                          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì™¸ë¶€ ë£¨í”„     â”‚      â”‚     ë‚¸ë¶€ ë£¨í”„      â”‚      â”‚   ë²„ì „ ì»¨íŠ¸ë¡¤     â”‚
â”‚ (Outer Loop)  â”‚      â”‚   (Inner Loop)   â”‚      â”‚  (Version Ctrl)  â”‚
â”‚               â”‚      â”‚                  â”‚      â”‚                  â”‚
â”‚ â€¢ Agent ê°œì„   â”‚â—€â”€â”€â”€â”€â”€â”‚ â€¢ ì œì¶œë¬¼ ê°œì„      â”‚      â”‚ â€¢ iteration      â”‚
â”‚ â€¢ Prompt ìµœì í™”â”‚      â”‚ â€¢ í’ˆì§ˆ í–¥ìƒ      â”‚      â”‚   commit        â”‚
â”‚ â€¢ Workflow    â”‚      â”‚ â€¢ ì‹¬ì‚¬ ê¸°ì¤€      â”‚      â”‚ â€¢ diff ì¶”ì       â”‚
â”‚   ì¬ì„¤ê³„      â”‚      â”‚   ì¶©ì¡±          â”‚      â”‚ â€¢ rollback       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ

```python
# ì‹œìŠ¤í…œ êµ¬ì¡°
MIRROR/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ meta_learner.py          # ë©”íƒ€ëŸ¬ë‹ ì—”ì§„
â”‚   â”œâ”€â”€ reflection_engine.py     # ë¦¬í”Œë ‰ì…˜ ì—”ì§„
â”‚   â”œâ”€â”€ version_controller.py    # ë²„ì „ ì»¨íŠ¸ë¡¤ëŸ¬
â”‚   â””â”€â”€ feedback_loop.py         # í”¼ë“œë°± ë£¨í”„ ê´€ë¦¬
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py            # ê¸°ë³¸ ì—ì´ì „íŠ¸ (self-improving)
â”‚   â”œâ”€â”€ research_agents/         # ì—°êµ¬ ìˆ˜í–‰ ì—ì´ì „íŠ¸ë“¤
â”‚   â””â”€â”€ system_agents/           # ì‹œìŠ¤í…œ ê´€ë¦¬ ì—ì´ì „íŠ¸ë“¤
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ episodic_memory.py       # ì—í”¼ì†Œë”• ë©”ëª¨ë¦¬ (iterationë³„)
â”‚   â”œâ”€â”€ semantic_memory.py       # ì‹œë§¨í‹± ë©”ëª¨ë¦¬ (ëˆ„ì  í•™ìŠµ)
â”‚   â””â”€â”€ procedural_memory.py     # í”„ë¡œì‹œì €ëŸ´ ë©”ëª¨ë¦¬ (workflow)
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ rubric_evaluator.py      # ì‹¬ì‚¬ ê¸°ì¤€ í‰ê°€ê¸°
â”‚   â”œâ”€â”€ ai_judges.py             # ë‹¤ì¤‘ AI ì‹¬ì‚¬ìœ„ì›
â”‚   â””â”€â”€ gap_analyzer.py          # Gap ë¶„ì„ê¸°
â””â”€â”€ outputs/
    â”œâ”€â”€ submissions/             # ì œì¶œë¬¼ (ë²„ì „ë³„)
    â”œâ”€â”€ agent_versions/          # ì—ì´ì „íŠ¸ ë²„ì „
    â””â”€â”€ learning_logs/           # í•™ìŠµ ë¡œê·¸
```

---

## 3. ì´ì¤‘ ë£¨í”„ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ (Dual-Loop Learning Mechanism)

### 3.1 ë‚¸ë¶€ ë£¨í”„: ì œì¶œë¬¼ ê°œì„  (Inner Loop)

```python
class InnerLoop:
    """ì œì¶œë¬¼ í’ˆì§ˆ ê°œì„  ë£¨í”„"""
    
    def iterate(self, current_submission, evaluation_result):
        """
        1. ì‹¬ì‚¬ ê²°ê³¼ ë¶„ì„
        2. ì•½ì  ì‹ë³„
        3. ê°œì„  ì „ëµ ìˆ˜ë¦½
        4. ì œì¶œë¬¼ ìˆ˜ì •
        5. í’ˆì§ˆ ê²€ì¦
        """
        weaknesses = self.identify_weaknesses(evaluation_result)
        strategy = self.generate_improvement_strategy(weaknesses)
        improved = self.apply_improvements(current_submission, strategy)
        return improved
```

### 3.2 ì™¸ë¶€ ë£¨í”„: ì—ì´ì „íŠ¸ ê°œì„  (Outer Loop)

```python
class OuterLoop:
    """ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ìì²´ ê°œì„  ë£¨í”„"""
    
    def meta_learn(self, iteration_history):
        """
        1. iteration íŒ¨í„´ ë¶„ì„
        2. ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¶„ì„
        3. ì•„í‚¤í…ì²˜ ê°œì„ ì  ì‹ë³„
        4. í”„ë¡¬í”„íŠ¸ ì „ëµ ìµœì í™”
        5. workflow ì¬êµ¬ì„±
        """
        patterns = self.analyze_patterns(iteration_history)
        agent_performance = self.evaluate_agents(patterns)
        improvements = self.identify_improvements(agent_performance)
        return self.apply_meta_improvements(improvements)
```

### 3.3 í†µí•© í”Œë¡œìš°

```
Iteration N ì‹œì‘
    â”‚
    â”œâ”€â”€â”€â–¶ [ë‚¸ë¶€ ë£¨í”„] ì œì¶œë¬¼ ê°œì„ 
    â”‚         â”‚
    â”‚         â”œâ”€â”€â–¶ AI Judge í‰ê°€ (3ê°œ ëª¨ë¸)
    â”‚         â”œâ”€â”€â–¶ Gap ë¶„ì„
    â”‚         â”œâ”€â”€â–¶ ê°œì„  ì „ëµ ìƒì„±
    â”‚         â””â”€â”€â–¶ ì œì¶œë¬¼ ìˆ˜ì •
    â”‚
    â”œâ”€â”€â”€â–¶ [ë²„ì „ ì»¨íŠ¸ë¡¤] commit ìƒì„±
    â”‚         â”‚
    â”‚         â”œâ”€â”€â–¶ diff ìƒì„±
    â”‚         â”œâ”€â”€â–¶ íƒœê·¸ ë¶€ì—¬ (vN.M)
    â”‚         â””â”€â”€â–¶ ë¡¤ë°± í¬ì¸íŠ¸ ì €ì¥
    â”‚
    â””â”€â”€â”€â–¶ [ì™¸ë¶€ ë£¨í”„] ë©”íƒ€ëŸ¬ë‹
              â”‚
              â”œâ”€â”€â–¶ iteration íŒ¨í„´ ë¶„ì„
              â”œâ”€â”€â–¶ ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€
              â”œâ”€â”€â–¶ ê°œì„  ì „ëµ ìƒì„±
              â””â”€â”€â–¶ ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ì—…ë°ì´íŠ¸

Iteration N+1 ì‹œì‘ (ê°œì„ ëœ ì—ì´ì „íŠ¸ë¡œ)
```

---

## 4. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ì„¤ê³„

### 4.1 ë©”íƒ€ëŸ¬ë‹ ì—”ì§„ (Meta-Learning Engine)

```python
class MetaLearningEngine:
    """
    iterationì„ ê±°ì¹˜ë©° ì‹œìŠ¤í…œ ìì²´ë¥¼ ê°œì„ í•˜ëŠ” ë©”íƒ€ëŸ¬ë‹ ì—”ì§„
    """
    
    def __init__(self):
        self.episodic_memory = EpisodicMemory()      # ë‹¨ê¸° ê¸°ì–µ
        self.semantic_memory = SemanticMemory()      # ì¥ê¸° ê¸°ì–µ
        self.procedural_memory = ProceduralMemory()  # ì ˆì°¨ ê¸°ì–µ
        
    def learn_from_iteration(self, iteration_data: IterationData):
        """
        iterationìœ¼ë¡œë¶€í„° í•™ìŠµ
        """
        # 1. ì—í”¼ì†Œë”• ë©”ëª¨ë¦¬ ì €ì¥
        self.episodic_memory.store(iteration_data)
        
        # 2. íŒ¨í„´ ì¶”ì¶œ ë° ì‹œë§¨í‹± ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        patterns = self.extract_patterns(iteration_data)
        self.semantic_memory.consolidate(patterns)
        
        # 3. í”„ë¡œì‹œì €ëŸ´ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        if iteration_data.success:
            self.procedural_memory.reinforce(iteration_data.workflow)
        else:
            self.procedural_memory.adjust(iteration_data.workflow, iteration_data.failure_reason)
    
    def generate_improvements(self) -> List[SystemImprovement]:
        """
        ê°œì„ ì‚¬í•­ ìƒì„±
        """
        improvements = []
        
        # ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ê°œì„ 
        agent_improvements = self.suggest_agent_improvements()
        improvements.extend(agent_improvements)
        
        # í”„ë¡¬í”„íŠ¸ ì „ëµ ê°œì„ 
        prompt_improvements = self.suggest_prompt_improvements()
        improvements.extend(prompt_improvements)
        
        # workflow ê°œì„ 
        workflow_improvements = self.suggest_workflow_improvements()
        improvements.extend(workflow_improvements)
        
        return improvements
    
    def suggest_agent_improvements(self) -> List[SystemImprovement]:
        """
        ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ê°œì„  ì œì•ˆ
        """
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        failure_patterns = self.episodic_memory.get_failure_patterns()
        
        improvements = []
        
        # íŠ¹ì • ì—ì´ì „íŠ¸ì˜ ë°˜ë³µì ì¸ ì‹¤íŒ¨
        for agent_name, failures in failure_patterns.by_agent.items():
            if len(failures) > 3:  # 3íšŒ ì´ìƒ ì‹¤íŒ¨
                improvements.append(SystemImprovement(
                    target=f"agent:{agent_name}",
                    action="decompose",
                    reason=f"{agent_name} has failed {len(failures)} times, needs decomposition"
                ))
        
        # ë³‘ëª© í˜„ìƒ ë¶„ì„
        bottlenecks = self.analyze_bottlenecks()
        for bottleneck in bottlenecks:
            improvements.append(SystemImprovement(
                target=f"workflow:{bottleneck.stage}",
                action="parallelize",
                reason=f"{bottleneck.stage} is causing delays"
            ))
        
        return improvements
```

### 4.2 ë¦¬í”Œë ‰ì…˜ ì—”ì§„ (Reflection Engine)

```python
class ReflectionEngine:
    """
    iteration ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  insightë¥¼ ì¶”ì¶œ
    """
    
    def reflect(self, iteration_result: IterationResult) -> ReflectionReport:
        """
        iterationì— ëŒ€í•œ ë¦¬í”Œë ‰ì…˜ ìˆ˜í–‰
        """
        report = ReflectionReport()
        
        # 1. ì„±ê³µ/ì‹¤íŒ¨ ë¶„ì„
        report.outcome_analysis = self.analyze_outcome(iteration_result)
        
        # 2. ì˜ì‚¬ê²°ì • ë¶„ì„
        report.decision_analysis = self.analyze_decisions(iteration_result)
        
        # 3. ëŒ€ì•ˆ ê²½ë¡œ íƒìƒ‰
        report.alternatives = self.explore_alternatives(iteration_result)
        
        # 4. í•™ìŠµ í¬ì¸íŠ¸ ì¶”ì¶œ
        report.learning_points = self.extract_learning_points(iteration_result)
        
        return report
    
    def analyze_outcome(self, result: IterationResult) -> OutcomeAnalysis:
        """
        ê²°ê³¼ ë¶„ì„
        """
        expected = result.expected_score
        actual = result.actual_score
        
        gap = expected - actual
        
        if gap > 10:
            severity = "critical"
        elif gap > 5:
            severity = "major"
        else:
            severity = "minor"
        
        return OutcomeAnalysis(
            expected=expected,
            actual=actual,
            gap=gap,
            severity=severity,
            root_causes=self.identify_root_causes(result, gap)
        )
    
    def identify_root_causes(self, result: IterationResult, gap: float) -> List[RootCause]:
        """
        ê·¼ë³¸ ì›ì¸ ë¶„ì„
        """
        causes = []
        
        # ì‹¬ì‚¬ ê¸°ì¤€ë³„ ë¶„ì„
        for criterion, score in result.scores.items():
            max_score = RUBRIC[criterion]['max']
            if score < max_score * 0.8:  # 80% ë¯¸ë§Œ
                causes.append(RootCause(
                    category=criterion,
                    score=score,
                    max=max_score,
                    possible_causes=self.investigate_cause(criterion, result)
                ))
        
        return causes
```

### 4.3 ë²„ì „ ì»¨íŠ¸ë¡¤ëŸ¬ (Version Controller)

```python
class VersionController:
    """
    iterationë§ˆë‹¤ commitì„ ìƒì„±í•˜ê³  ë²„ì „ ê´€ë¦¬
    """
    
    def __init__(self, repo_path: str):
        self.repo = git.Repo(repo_path)
        self.version_tags = []
        
    def commit_iteration(self, iteration_num: int, changes: Changes) -> Commit:
        """
        iteration ê²°ê³¼ë¥¼ commit
        """
        # ë³€ê²½ì‚¬í•­ ìŠ¤í…Œì´ì§•
        self.stage_changes(changes)
        
        # ì»¤ë°‹ ë©”ì‹œì§€ ìƒì„±
        commit_message = self.generate_commit_message(iteration_num, changes)
        
        # ì»¤ë°‹ ìƒì„±
        commit = self.repo.commit(commit_message)
        
        # íƒœê·¸ ìƒì„±
        tag = f"v{iteration_num}.0"
        self.repo.create_tag(tag, commit)
        self.version_tags.append(tag)
        
        # ë³€ê²½ ë¡œê·¸ ìƒì„±
        self.generate_changelog(iteration_num, changes)
        
        return commit
    
    def generate_commit_message(self, iteration: int, changes: Changes) -> str:
        """
        ì»¤ë°‹ ë©”ì‹œì§€ ìƒì„±
        """
        lines = [
            f"[Iteration {iteration}] Submission Improvement",
            "",
            f"Score Change: {changes.score_change:+.1f}",
            f"AI Contribution: {changes.ai_contribution}%",
            "",
            "Improvements:",
        ]
        
        for improvement in changes.improvements:
            lines.append(f"  - {improvement.category}: {improvement.description}")
        
        lines.extend([
            "",
            "Agent Updates:",
        ])
        
        for update in changes.agent_updates:
            lines.append(f"  - {update.agent}: {update.change_type}")
        
        return "\n".join(lines)
    
    def rollback_to(self, tag: str) -> None:
        """
        íŠ¹ì • ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
        """
        self.repo.git.checkout(tag)
        
    def get_diff(self, tag1: str, tag2: str) -> Diff:
        """
        ë‘ ë²„ì „ ê°„ diff ìƒì„±
        """
        return self.repo.git.diff(tag1, tag2)
```

### 4.4 ë‹¤ì¤‘ AI ì‹¬ì‚¬ìœ„ì› (Multi-AI Judges)

```python
class MultiAIJudges:
    """
    3ê°œ ì´ìƒ AI ëª¨ë¸ì„ í™œìš©í•œ ë‹¤ì¤‘ ì‹¬ì‚¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.judges = {
            'claude': ClaudeJudge(model="claude-3-5-sonnet-20241022"),
            'gpt4': GPT4Judge(model="gpt-4"),
            'gemini': GeminiJudge(model="gemini-pro"),
        }
        
    def evaluate(self, submission: Submission) -> MultiJudgeResult:
        """
        ë‹¤ì¤‘ ì‹¬ì‚¬ ìˆ˜í–‰
        """
        results = {}
        
        # ê° ì‹¬ì‚¬ìœ„ì› í‰ê°€
        for name, judge in self.judges.items():
            logger.info(f"Judge {name} evaluating...")
            results[name] = judge.evaluate(submission)
        
        # ê²°ê³¼ ì§‘ê³„
        aggregated = self.aggregate_results(results)
        
        # ë¶ˆì¼ì¹˜ ë¶„ì„
        discrepancies = self.analyze_discrepancies(results)
        
        return MultiJudgeResult(
            individual_results=results,
            aggregated=aggregated,
            discrepancies=discrepancies,
            confidence=self.calculate_confidence(results)
        )
    
    def aggregate_results(self, results: Dict[str, JudgeResult]) -> AggregatedScore:
        """
        ì‹¬ì‚¬ ê²°ê³¼ ì§‘ê³„ (ì•™ìƒë¸”)
        """
        aggregated = {}
        
        for criterion in RUBRIC.keys():
            scores = [r.scores[criterion] for r in results.values()]
            
            # ì¤‘ì•™ê°’ ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•ê±´)
            aggregated[criterion] = {
                'median': np.median(scores),
                'mean': np.mean(scores),
                'std': np.std(scores),
                'min': min(scores),
                'max': max(scores),
                'consensus': self.check_consensus(scores)
            }
        
        return aggregated
    
    def analyze_discrepancies(self, results: Dict[str, JudgeResult]) -> List[Discrepancy]:
        """
        ì‹¬ì‚¬ìœ„ì› ê°„ ë¶ˆì¼ì¹˜ ë¶„ì„
        """
        discrepancies = []
        
        for criterion in RUBRIC.keys():
            scores = {name: r.scores[criterion] for name, r in results.items()}
            
            max_diff = max(scores.values()) - min(scores.values())
            
            if max_diff > 5:  # 5ì  ì´ìƒ ì°¨ì´
                discrepancies.append(Discrepancy(
                    criterion=criterion,
                    scores=scores,
                    max_difference=max_diff,
                    possible_reasons=self.investigate_discrepancy(criterion, scores)
                ))
        
        return discrepancies
```

---

## 5. Self-Improving Agent ì„¤ê³„

### 5.1 ê¸°ë³¸ Self-Improving Agent

```python
class SelfImprovingAgent(ABC):
    """
    ìŠ¤ìŠ¤ë¡œ ê°œì„ í•˜ëŠ” ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤
    """
    
    def __init__(self, name: str):
        self.name = name
        self.version = "1.0.0"
        self.performance_history = []
        self.prompt_strategies = []
        self.current_strategy = None
        
    @abstractmethod
    def execute(self, task: Task) -> Result:
        pass
    
    def improve(self, feedback: Feedback) -> None:
        """
        í”¼ë“œë°±ì„ ë°›ì•„ ìŠ¤ìŠ¤ë¡œ ê°œì„ 
        """
        # ì„±ëŠ¥ ê¸°ë¡
        self.performance_history.append({
            'timestamp': datetime.now(),
            'feedback': feedback,
            'strategy': self.current_strategy
        })
        
        # ê°œì„  ì „ëµ ì„ íƒ
        if feedback.score < 0.7:  # 70% ë¯¸ë§Œ
            self.adapt_strategy(feedback)
        
        # í”„ë¡¬í”„íŠ¸ ìµœì í™”
        self.optimize_prompt(feedback)
        
        # ë²„ì „ ì—…ë°ì´íŠ¸
        self.update_version()
    
    def adapt_strategy(self, feedback: Feedback) -> None:
        """
        ì „ëµ ì ì‘
        """
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        patterns = self.analyze_failure_patterns()
        
        # ìƒˆë¡œìš´ ì „ëµ ìƒì„±
        new_strategy = self.generate_new_strategy(patterns)
        
        # ì „ëµ í…ŒìŠ¤íŠ¸
        if self.test_strategy(new_strategy):
            self.current_strategy = new_strategy
            self.prompt_strategies.append(new_strategy)
    
    def optimize_prompt(self, feedback: Feedback) -> None:
        """
        í”„ë¡¬í”„íŠ¸ ìµœì í™”
        """
        # Few-shot ì˜ˆì œ ì—…ë°ì´íŠ¸
        if feedback.type == 'fewshot_insufficient':
            self.add_fewshot_examples(feedback.examples)
        
        # Chain-of-Thought ê°œì„ 
        if feedback.type == 'reasoning_unclear':
            self.enhance_cot_prompt()
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •
        if feedback.type == 'context_overflow':
            self.adjust_context_window()
    
    def update_version(self) -> None:
        """
        Semantic versioning
        """
        parts = self.version.split('.')
        major, minor, patch = int(parts[0]), int(parts[1]), int(parts[2])
        
        # Major: ì•„í‚¤í…ì²˜ ë³€ê²½
        # Minor: ì „ëµ ë³€ê²½
        # Patch: í”„ë¡¬í”„íŠ¸ ì¡°ì •
        
        if self.architecture_changed():
            major += 1
            minor = 0
            patch = 0
        elif self.strategy_changed():
            minor += 1
            patch = 0
        else:
            patch += 1
        
        self.version = f"{major}.{minor}.{patch}"
```

---

## 6. ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ (Memory System)

### 6.1 3ê³„ì¸µ ë©”ëª¨ë¦¬ êµ¬ì¡°

```python
class MemorySystem:
    """
    ì¸ì§€ ì•„í‚¤í…ì²˜ ê¸°ë°˜ 3ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        # ì—í”¼ì†Œë”• ë©”ëª¨ë¦¬: ê°œë³„ iterationì˜ êµ¬ì²´ì  ê²½í—˜
        self.episodic = EpisodicMemory(
            storage=VectorStore(),
            retention_policy='last_10_iterations'
        )
        
        # ì‹œë§¨í‹± ë©”ëª¨ë¦¬: ì¶”ìƒí™”ëœ ì§€ì‹ê³¼ íŒ¨í„´
        self.semantic = SemanticMemory(
            storage=KnowledgeGraph(),
            consolidation_interval='every_5_iterations'
        )
        
        # í”„ë¡œì‹œì €ëŸ´ ë©”ëª¨ë¦¬: workflowì™€ ì ˆì°¨ì  ì§€ì‹
        self.procedural = ProceduralMemory(
            storage=RuleEngine(),
            adaptation_rate=0.1
        )
    
    def store_experience(self, iteration: IterationData) -> None:
        """
        ê²½í—˜ ì €ì¥
        """
        # ì—í”¼ì†Œë”• ë©”ëª¨ë¦¬ì— ì €ì¥
        self.episodic.store(iteration)
        
        # ì¼ì • ì£¼ê¸°ë¡œ ì‹œë§¨í‹± ë©”ëª¨ë¦¬ì— í†µí•©
        if self.should_consolidate():
            self.consolidate_to_semantic()
    
    def consolidate_to_semantic(self) -> None:
        """
        ì—í”¼ì†Œë”• â†’ ì‹œë§¨í‹± í†µí•©
        """
        recent_episodes = self.episodic.get_recent(k=5)
        
        # íŒ¨í„´ ì¶”ì¶œ
        patterns = self.extract_patterns(recent_episodes)
        
        # ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        for pattern in patterns:
            self.semantic.add_knowledge(
                subject=pattern.subject,
                predicate=pattern.predicate,
                object=pattern.object,
                confidence=pattern.confidence
            )
    
    def retrieve_relevant(self, query: Query, context: Context) -> RetrievedInfo:
        """
        ìƒí™©ì— ë§ëŠ” ì •ë³´ ê²€ìƒ‰
        """
        # ëª¨ë“  ë©”ëª¨ë¦¬ ê³„ì¸µì—ì„œ ê²€ìƒ‰
        episodic_results = self.episodic.search(query, top_k=3)
        semantic_results = self.semantic.query(query)
        procedural_results = self.procedural.match(context)
        
        # ê²°ê³¼ ìœµí•©
        return self.fuse_results(
            episodic=episodic_results,
            semantic=semantic_results,
            procedural=procedural_results
        )
```

---

## 7. ì‹¤í–‰ ì›Œí¬í”Œë¡œìš°

### 7.1 ë©”ì¸ ë£¨í”„

```python
class MIRROREngine:
    """
    MIRROR ì‹œìŠ¤í…œ ë©”ì¸ ì—”ì§„
    """
    
    def __init__(self):
        self.meta_learner = MetaLearningEngine()
        self.reflection = ReflectionEngine()
        self.version_ctrl = VersionController()
        self.judges = MultiAIJudges()
        self.agents = self.initialize_agents()
        
    def run(self, max_iterations: int = 20, target_score: float = 85) -> Submission:
        """
        ë©”ì¸ ì‹¤í–‰ ë£¨í”„
        """
        for iteration in range(1, max_iterations + 1):
            logger.info(f"=== Iteration {iteration} ===")
            
            # 1. ì—°êµ¬ ìˆ˜í–‰
            submission = self.execute_research()
            
            # 2. ë‹¤ì¤‘ AI ì‹¬ì‚¬
            evaluation = self.judges.evaluate(submission)
            
            # 3. ëª©í‘œ ë‹¬ì„± í™•ì¸
            if evaluation.aggregated['total'] >= target_score:
                logger.info(f"Target achieved at iteration {iteration}!")
                self.finalize(submission, evaluation)
                return submission
            
            # 4. ë¦¬í”Œë ‰ì…˜
            reflection = self.reflection.reflect(
                IterationResult(
                    submission=submission,
                    evaluation=evaluation,
                    iteration=iteration
                )
            )
            
            # 5. ë‚¸ë¶€ ë£¨í”„: ì œì¶œë¬¼ ê°œì„ 
            improved_submission = self.improve_submission(
                submission, evaluation, reflection
            )
            
            # 6. ë²„ì „ ì»¨íŠ¸ë¡¤: commit
            self.version_ctrl.commit_iteration(iteration, Changes(
                submission=improved_submission,
                evaluation=evaluation,
                reflection=reflection
            ))
            
            # 7. ë©”íƒ€ëŸ¬ë‹: ì‹œìŠ¤í…œ ê°œì„ 
            if iteration % 3 == 0:  # 3 iterationë§ˆë‹¤
                improvements = self.meta_learner.generate_improvements()
                self.apply_system_improvements(improvements)
            
            # 8. í•™ìŠµ
            self.meta_learner.learn_from_iteration(IterationData(
                iteration=iteration,
                submission=improved_submission,
                evaluation=evaluation,
                reflection=reflection
            ))
        
        # ìµœëŒ€ iteration ë„ë‹¬
        logger.warning("Max iterations reached")
        return self.get_best_submission()
    
    def execute_research(self) -> Submission:
        """
        ì—°êµ¬ ìˆ˜í–‰
        """
        # ê° ì—ì´ì „íŠ¸ê°€ self-improving í•˜ê²Œ ë™ì‘
        literature = self.agents['literature'].review()
        hypothesis = self.agents['hypothesis'].generate(literature)
        data = self.agents['data'].analyze(hypothesis)
        paper = self.agents['writer'].write(data)
        ai_log = self.agents['logger'].compile()
        
        return Submission(
            paper=paper,
            ai_usage=ai_log,
            data_list=data.metadata
        )
    
    def apply_system_improvements(self, improvements: List[SystemImprovement]) -> None:
        """
        ì‹œìŠ¤í…œ ê°œì„ ì‚¬í•­ ì ìš©
        """
        for improvement in improvements:
            logger.info(f"Applying improvement: {improvement}")
            
            if improvement.target.startswith('agent:'):
                agent_name = improvement.target.split(':')[1]
                self.agents[agent_name].apply_improvement(improvement)
            
            elif improvement.target.startswith('workflow:'):
                self.reconfigure_workflow(improvement)
            
            elif improvement.target.startswith('prompt:'):
                self.update_prompt_strategy(improvement)
```

---

## 8. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
ai_co_scientist_v2/
â”œâ”€â”€ mirror/                          # MIRROR ì‹œìŠ¤í…œ ì½”ì–´
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ engine.py                    # ë©”ì¸ ì—”ì§„
â”‚   â”œâ”€â”€ meta_learning.py             # ë©”íƒ€ëŸ¬ë‹ ì—”ì§„
â”‚   â”œâ”€â”€ reflection.py                # ë¦¬í”Œë ‰ì…˜ ì—”ì§„
â”‚   â”œâ”€â”€ version_control.py           # ë²„ì „ ì»¨íŠ¸ë¡¤ëŸ¬
â”‚   â””â”€â”€ feedback_loop.py             # í”¼ë“œë°± ë£¨í”„
â”œâ”€â”€ agents/                          # Self-Improving Agents
â”‚   â”œâ”€â”€ base.py                      # ê¸°ë³¸ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ literature_agent.py          # ë¬¸í—Œ ì¡°ì‚¬
â”‚   â”œâ”€â”€ hypothesis_agent.py          # ê°€ì„¤ ìƒì„±
â”‚   â”œâ”€â”€ data_agent.py                # ë°ì´í„° ë¶„ì„
â”‚   â”œâ”€â”€ writing_agent.py             # ë…¼ë¬¸ ì‘ì„±
â”‚   â”œâ”€â”€ logging_agent.py             # AI í™œìš© ë¡œê¹…
â”‚   â””â”€â”€ judge_agents/                # AI ì‹¬ì‚¬ìœ„ì›
â”‚       â”œâ”€â”€ claude_judge.py
â”‚       â”œâ”€â”€ gpt4_judge.py
â”‚       â””â”€â”€ gemini_judge.py
â”œâ”€â”€ memory/                          # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ episodic.py                  # ì—í”¼ì†Œë”• ë©”ëª¨ë¦¬
â”‚   â”œâ”€â”€ semantic.py                  # ì‹œë§¨í‹± ë©”ëª¨ë¦¬
â”‚   â””â”€â”€ procedural.py                # í”„ë¡œì‹œì €ëŸ´ ë©”ëª¨ë¦¬
â”œâ”€â”€ evaluation/                      # í‰ê°€ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ rubric.py                    # ì‹¬ì‚¬ ê¸°ì¤€
â”‚   â”œâ”€â”€ multi_judge.py               # ë‹¤ì¤‘ ì‹¬ì‚¬
â”‚   â””â”€â”€ gap_analyzer.py              # Gap ë¶„ì„
â”œâ”€â”€ prompts/                         # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”‚   â”œâ”€â”€ templates/                   # ê¸°ë³¸ í…œí”Œë¦¿
â”‚   â”œâ”€â”€ strategies/                  # ì „ëµë³„ í”„ë¡¬í”„íŠ¸
â”‚   â””â”€â”€ evolved/                     # ì§„í™”ëœ í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ submissions/                     # ì œì¶œë¬¼ (ë²„ì „ë³„)
â”‚   â”œâ”€â”€ v1.0.0/                      # iteration 1
â”‚   â”œâ”€â”€ v1.1.0/                      # iteration 2
â”‚   â””â”€â”€ ...
â”œâ”€â”€ learning_logs/                   # í•™ìŠµ ë¡œê·¸
â”‚   â”œâ”€â”€ episodic/                    # ì—í”¼ì†Œë”• ê¸°ë¡
â”‚   â”œâ”€â”€ semantic/                    # ì‹œë§¨í‹± ì§€ì‹
â”‚   â””â”€â”€ reflections/                 # ë¦¬í”Œë ‰ì…˜ ê¸°ë¡
â”œâ”€â”€ config/                          # ì„¤ì •
â”‚   â”œâ”€â”€ agents.yaml                  # ì—ì´ì „íŠ¸ ì„¤ì •
â”‚   â”œâ”€â”€ rubric.yaml                  # ì‹¬ì‚¬ ê¸°ì¤€
â”‚   â””â”€â”€ models.yaml                  # ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ main.py                          # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## 9. Commit ë©”ì‹œì§€ ê·œì•½

```
[Iteration {N}] {ì œëª©}

Score: {ì´ì „} â†’ {í˜„ì¬} ({ë³€í™”ëŸ‰:+.1f})
AI Contribution: {ç™¾åˆ†æ¯”}%

## Improvements
- {ê°œì„ ì‚¬í•­ 1}
- {ê°œì„ ì‚¬í•­ 2}

## Agent Updates
- {ì—ì´ì „íŠ¸ëª…}: {ë³€ê²½ë‚´ìš©}

## Reflection
{ë¦¬í”Œë ‰ì…˜ ìš”ì•½}

## Next Steps
{ë‹¤ìŒ iteration ê³„íš}
```

---

## 10. í•µì‹¬ í˜ì‹ ì  ìš”ì•½

| ê¸°ì¡´ ì‹œìŠ¤í…œ | MIRROR ì‹œìŠ¤í…œ |
|-------------|---------------|
| ì •ì  ì—ì´ì „íŠ¸ | **Self-Improving Agents** |
| ë‹¨ì¼ í”¼ë“œë°± ë£¨í”„ | **ì´ì¤‘ ë£¨í”„ í•™ìŠµ** (ì œì¶œë¬¼ + ì‹œìŠ¤í…œ) |
| No ë²„ì „ ê´€ë¦¬ | **iterationë§ˆë‹¤ commit** |
| ë‹¨ì¼ AI ëª¨ë¸ | **3ê°œ ì´ìƒ ë‹¤ì¤‘ AI ì‹¬ì‚¬** |
| Mock ë°ì´í„° | **ì‹¤ì œ ë°ì´í„° ë¶„ì„** |
| ê³ ì •ëœ í”„ë¡¬í”„íŠ¸ | **ì§„í™”í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì „ëµ** |
| No ë©”íƒ€ëŸ¬ë‹ | **3ê³„ì¸µ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ** |

---

*ì´ ì‹œìŠ¤í…œì€ 2026 AI Co-Scientist Challenge Koreaì˜ Track 1 ì°¸ê°€ë¥¼ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.*
*ì„¤ê³„ ì›ì¹™: "ì‹œìŠ¤í…œì´ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹œìŠ¤í…œì´ ìŠ¤ìŠ¤ë¡œë¥¼ ê°œì„ í•˜ë©° ì—°êµ¬ë¥¼ ìˆ˜í–‰í•œë‹¤"*
